<!doctype html><html dir=ltr lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=5"><title>Job Alerts | Aurora
</title><link rel=icon href=/favicon.ico type=image/x-icon><link rel=preload href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css as=style crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.design-system.alpha.canada.ca/@cdssnc/gcds-utility@1.10.0/dist/gcds-utility.min.css><link rel=stylesheet href=https://cdn.design-system.alpha.canada.ca/@cdssnc/gcds-components@0.42.0/dist/gcds/gcds.css><script async type=module src=https://cdn.design-system.alpha.canada.ca/@cdssnc/gcds-components@0.42.0/dist/gcds/gcds.esm.js></script><link rel=stylesheet href=/scss/main.css><link rel=stylesheet href=/scss/custom.css><style>html{font-size:16px}body{font-size:var(--gcds-font-sizes-text);line-height:var(--gcds-line-heights-text);color:var(--gcds-color-grayscale-1000);font-family:var(--gcds-font-families-body),sans-serif;margin:0;animation:fade .05s forwards ease-in-out}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/glightbox/dist/css/glightbox.min.css></head><body style=margin:0 class=character-limit><main><gcds-container centered size=md><gcds-text size=caption style="text-align: center;"><strong>Alpha</strong>&nbsp;&nbsp;This is an experimental service.</gcds-text>
</gcds-container><gcds-header lang=en lang-href signature-variant=colour skip-to-href=#><gcds-search lang=en slot=search action=/sr/srb.html placeholder=Aurora></gcds-search><gcds-top-nav slot=menu label="Top navigation" alignment=right><gcds-nav-link href=/ slot=home>GC Cloud One: Aurora
</gcds-nav-link><gcds-nav-group open-trigger=Platform menu-label=Platform><gcds-nav-link href=/proposal>Proposal
</gcds-nav-link><gcds-nav-link href=/vision>Vision
</gcds-nav-link><gcds-nav-link href=/architecture>Architecture
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger=Team menu-label=Team><gcds-nav-link href=/team/sop>Standard Operating Procedures
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/>Monitoring
</gcds-nav-link><gcds-nav-link href=/team/appendix>Appendix
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger=Community menu-label=Community><gcds-nav-link href=/get-involved>Get involved
</gcds-nav-link><gcds-nav-link href=/rules-of-engagement>Rules of Engagement
</gcds-nav-link><gcds-nav-link href=/technical-advisory-group>Technical Advisory Group
</gcds-nav-link></gcds-nav-group><gcds-nav-link href=/contact slot>Contact us
</gcds-nav-link></gcds-top-nav><gcds-breadcrumbs slot=breadcrumb><gcds-breadcrumbs-item href="https://hosting-services-hebergement.canada.ca/s/gc-cloud-one?language=en_US">GC Cloud One</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/>Aurora</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/team/>Team Guide</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/team/monitoring-alerts/>Monitoring and Alerting</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/team/monitoring-alerts/alert-namespace-level/>Namespace Level Alerts</gcds-breadcrumbs-item>
</gcds-breadcrumbs></gcds-header><gcds-container size=xl main-container centered tag=main><gcds-grid tag=section columns=1fr columns-desktop="
        1fr 3fr
      " align-items=start class=hydrated><aside role=complementary><gcds-side-nav label="Team Guide"><gcds-nav-link href=/team/>Team Guide
</gcds-nav-link><gcds-nav-group open-trigger="Standard Operating Procedures" menu-label="Standard Operating Procedures"><gcds-nav-link href=/team/standard-operating-procedures/>Standard Operating Procedures
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/backup-disaster-recovery/>Backup and Disaster Recovery
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/bootstrap-cluster/>Bootstrap Cluster
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/infrastructure-and-configuration-management/>Infrastructure and Configuration Management
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/issue-naming/>Issue and PR Naming Conventions
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/enterprise-landing-zone/>Onboarding process for the Enterprise Landing Zone
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger="Monitoring and Alerting" menu-label="Monitoring and Alerting" open><gcds-nav-link href=/team/monitoring-alerts/>Monitoring and Alerting
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/prometheus/>Prometheus
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/prometheus-operator/>Prometheus Operator
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alertmanager/>Alertmanager
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/blackbox-exporter/>Blackbox Exporter
</gcds-nav-link><gcds-nav-group open-trigger="Cluster Level Alerts" menu-label="Cluster Level Alerts"><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/>Cluster Level Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/cert-manager/>Cert Manager Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/node/>Node Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/node-pool-pod-capacity/>Node Pool Capacity Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/probe-failure/>Probe Failure Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/prometheus-storage-low/>Prometheus Storage Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/ssl-cert-expiring-soon/>SSL Certificate Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/velero/>Velero Alerts
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger="Namespace Level Alerts" menu-label="Namespace Level Alerts" open><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/>Namespace Level Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/container/>Container Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/job/ current=true>Job Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/persistent-volume-claims/>Persistent Volume Claim Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/pod-not-ready/>Pod Alerts
</gcds-nav-link></gcds-nav-group></gcds-nav-group><gcds-nav-group open-trigger=Appendix menu-label=Appendix><gcds-nav-link href=/team/appendix/>Appendix
</gcds-nav-link><gcds-nav-link href=/team/appendix/acronyms/>Acronyms
</gcds-nav-link><gcds-nav-link href=/team/appendix/lexicon/>Lexicon</gcds-nav-link></gcds-nav-group></gcds-side-nav></aside><section class=mb-400><gcds-heading tag=h1>Job Alerts</gcds-heading>
<gcds-alert alert-role=danger container=full heading="Avis de traduction" hide-close-btn=true hide-role-icon=false is-fixed=false class="hydrated mb-400"><gcds-text>Veuillez noter que ce document est actuellement en cours de développement actif et pourrait être sujet à des révisions. Une fois terminé, il sera entièrement traduit en français et mis à disposition dans sa version finale.</gcds-text>
</gcds-alert><gcds-heading tag=h2 id=alert-completedjobsnotcleared character-limit=true>Alert: CompletedJobsNotCleared</gcds-heading>
<gcds-text character-limit=true>This alert occurs at the namespace-level within a cluster and will be triggered when more than <em>20</em> completed jobs within a particular namespace are older than 24 hours. This alert indicates that certain completed jobs may need to be cleaned up or that there is an issue preventing jobs from completing successfully.</gcds-text>
<gcds-heading tag=h3 id=resolution-process character-limit=true>Resolution Process</gcds-heading>
<gcds-text character-limit=true>As always, troubleshooting and resolution of these errors can be done via kubectl, through the Lens IDE, or a combination of both depending on the developers&rsquo; preferences. To resolve this error, you will need to navigate to the specific cluster and namespace that is experiencing the issue.</gcds-text>
<gcds-text character-limit=true>If the completed jobs identified are backed by a CronJob resource and they seem to be piling up quickly, you can configure
<gcds-link href=https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#jobs-history-limits>job history limits</gcds-link> and let Kubernetes automatically clean up resources based on this history limit.</gcds-text>
<gcds-text character-limit=true>Otherwise, you will need to clean up the completed jobs within the namespace manually (i.e. deleting 1-by-1), or through an automated process. Additionally, you can also check the configuration of what is generating these jobs (i.e. the Helm chart for a particular application) to see if it has settings available for job management.</gcds-text>
<gcds-heading tag=h4 id=option-1-manual-cleanup character-limit=true>Option 1: Manual cleanup</gcds-heading>
<gcds-text character-limit=true>You can manually clean up the completed jobs individually, with <em>kubectl</em> or through the <em>Lens</em>.</gcds-text>
<gcds-text character-limit=true>To manually cleanup the Jobs using <em>Lens</em>, simply navigate to the appropriate namespace and resources, and utlize built-in functionality to perform deletions and/or modifications of resources.</gcds-text>
<gcds-text character-limit=true>To manually cleanup the Jobs using <em>kubectl</em>, follow the steps defined below:</gcds-text><ul><li><gcds-text character-limit=true>List all of the completed Jobs in the namespace for the Job being identified by the alert. For the grep component, you can use part of the Job name as an identifier:
<code>kubectl -n &lt;namespace> get jobs | grep "&lt;Job-Name-Identifier>"</code></gcds-text></li><li><gcds-text character-limit=true>Delete each completed Job from the list above, using the Job&rsquo;s name:
<code>kubectl -n &lt;namespace> delete jobs &lt;Job-Name-Here></code></gcds-text></li><li><gcds-text character-limit=true>Alternatively, you can combine these commands together to delete all successfully completed jobs within a namespace:
<code>kubectl -n &lt;namespace> delete jobs --field-selector status.successful=1</code></gcds-text></li></ul><gcds-heading tag=h4 id=option-2-automated-cleanup-with-ttl-mechanism-for-finished-jobs character-limit=true>Option 2: Automated cleanup with TTL mechanism for finished Jobs</gcds-heading>
<gcds-text character-limit=true>In Kubernetes v1.21, there is an additional TTL (time-to-live) mechanism that could be leveraged to clean up jobs automatically. This TTL mechanism is provided by a
<gcds-link href=https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/>TTL controller</gcds-link> for finished resources, by specifying the <code>.spec.ttlSecondsAfterFinished</code> field of the Job.</gcds-text>
<gcds-text character-limit=true>When the TTL controller cleans up the Job, it will delete the Job cascadingly, i.e. delete its dependent objects, such as Pods, together with the Job. Note that when the Job is deleted, its lifecycle guarantees, such as finalizers, will be honoured.</gcds-text>
<gcds-text character-limit=true>For example, within the Job resource, you specify the ttlSecondsAfterFinished value as seen in the snippet below:</gcds-text><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>batch/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Job</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>example-job</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ttlSecondsAfterFinished</span>: <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      <span style=color:#ae81ff>...</span>
</span></span></code></pre></div><gcds-text character-limit=true>This could also be relevant for failed Jobs too. If the above mechanism is used, we may want to set the <code>ttlSecondsAfterFinished</code> value to a more generous number (days, or weeks) depending on established retention policies.</gcds-text>
<gcds-heading tag=h2 id=alert-jobfailed character-limit=true>Alert: JobFailed</gcds-heading>
<gcds-text character-limit=true>This alert occurs at the namespace-level within a cluster. This alert will be triggered when a Kubernetes Job has failed to complete. It is based on a Job having the status <code>Failed</code>, and will therefore repeat until the failed Job is deleted from the cluster.</gcds-text>
<gcds-text character-limit=true>By default, a
<gcds-link href=https://kubernetes.io/docs/concepts/workloads/controllers/job/>Kubernetes Job</gcds-link> will run uninterrupted unless a Pod fails, or a container exits in error. At this point, a Job defers to a specified
<gcds-link href=https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy>backoff failure policy</gcds-link>. Once the defined <em>backoff limit</em> is reached, the Job will be marked as failed and any running Pods will be terminated.</gcds-text>
<gcds-heading tag=h3 id=resolution-process-1 character-limit=true>Resolution Process</gcds-heading>
<gcds-text character-limit=true>As always, troubleshooting and resolution of these errors can be done via <em>kubectl</em>, through the <em>Lens IDE</em>, or a combination of both depending on the developers&rsquo; preferences. To resolve this error, you will need to navigate to the specific cluster and namespace that is experiencing the issue.</gcds-text>
<gcds-text character-limit=true>The following steps can be used to determine the root cause as well as establishing a solution:</gcds-text><ol><li><gcds-text character-limit=true>Navigate to the appropriate cluster that is experiencing the issue.</gcds-text><ul><li>Set your KUBECONFIG, or navigate to the cluster in Lens.</li></ul></li><li><gcds-text character-limit=true>Analyze the failed Job. View the Job resource&rsquo;s <em>status</em> section. Here, there may have information regarding the root cause given by the <em>reason</em> and <em>message</em> fields:</gcds-text>
<gcds-text character-limit=true><code>kubectl -n &lt;namespace> describe job &lt;jobName></code></gcds-text><ul><li>For example, you may see information such as:<ul><li>&ldquo;reason: DeadlineExceeded , message: Job was active longer than specific deadline&rdquo;</li><li>&ldquo;reason: BackoffLimitExceeded , message: Job has reached the specified backoff limit&rdquo;</li></ul></li></ul></li><li><gcds-text character-limit=true>Determine the underlying root cause.</gcds-text><ul><li>From the analysis above, determine the underlying root cause related to the reason. You may need to research online, or collaborate with team members who may have previous experience.</li></ul></li><li><gcds-text character-limit=true>Implement the solution. Once the root cause has been identified, prepare and implement the appropriate solution. This may include:</gcds-text><ul><li>Increasing resources for the Job (cpu, memory)</li><li>Modifying the spec&rsquo;s <em>backoffLimit</em> to increase the number of retries before considering a Job as failed<ul><li>In cases where we know for certain a Job may take a long time and timeout, this could be a solution.</li></ul></li></ul></li><li><gcds-text character-limit=true>(If applicable) Delete or remove old Jobs that are stuck in the <em>JobFailed</em> state.</gcds-text><ul><li>Recall that the alert will keep firing until this Job is removed.</li><li>Once the issue has been resolved, you can delete old Jobs to remediate the alert.</li></ul></li></ol><gcds-heading tag=h3 id=additional-troubleshooting character-limit=true>Additional Troubleshooting</gcds-heading><ul><li><gcds-text character-limit=true>Depending on the situation, you may or may not have access to <em>Pod</em> logs related to the Job. You will see these Pods available while the Job is running. However, once a Job has failed its Pods will disappear.</gcds-text><ul><li>You may be able to catch the pods at the right time, to analyze pod logs, or to shell into the containers for further troubleshooting. However, this should not be the priority during the above Resolution Process</li></ul></li><li><gcds-text character-limit=true>Most Kubernetes Jobs within Aurora are usually controlled by a simiarily named <em>CronJob</em> resource. Additional troubleshooting can be done using the CronJob resource:</gcds-text><ul><li><gcds-text character-limit=true>Inspect the resource using Lens or kubectl: <code>kubectl -n &lt;namespace> describe CronJob &lt;CronJobName></code></gcds-text><ul><li>You can view information such as the <em>schedule</em>, <em>lastSuccesfulTime</em>, <em>lastScheduledTime</em>, the <em>backoffLimit</em> value, as well as the <em>successfulJobsHistoryLimit</em> and <em>failedJobsHistoryLimit</em> values.</li></ul></li><li><gcds-text character-limit=true>Analyze the jobs history:</gcds-text><ul><li>When did the job start failing? How many times has it failed? When was the last successful job? Is this the first occurence of a failure?</li></ul></li><li><gcds-text character-limit=true>Depending on the CronJob, you could also temporarily modify the Job to run more often so that you could analyze pod logs while the job runs</gcds-text><ul><li>Modify the <em>schedule</em> to make the CronJob run based on your specifications, and analyze the pod logs.</li></ul></li></ul></li></ul><gcds-heading tag=h2 id=alert-jobincomplete character-limit=true>Alert: JobIncomplete</gcds-heading>
<gcds-text character-limit=true>This alert occurs at the namespace-level within a cluster and will be triggered when a given Job is taking more than 12 hours to complete. To troubleshoot this Alert, you will need to determine why the Job may be failing to complete successfully, as well as apply remediation steps to fix the Job.</gcds-text>
<gcds-text character-limit=true>This Alert could highlight an issue with the Job resource itself, or an underlying issue related to cluster resources, or Job scheduling etc.</gcds-text>
<gcds-heading tag=h3 id=resolution-process-2 character-limit=true>Resolution Process</gcds-heading>
<gcds-text character-limit=true>As always, troubleshooting and resolution of these errors can be done via <em>kubectl</em>, through the <em>Lens IDE</em>, or a combination of both depending on the developers&rsquo; preferences. Recall that in some cases however, it may be ideal to double-check on the Job resource through <em>kubectl</em> (kubectl describe), as Lens may not always display the most up-to-date information.</gcds-text>
<gcds-text character-limit=true>To resolve this error, you will need to navigate to the specific cluster and namespace that is experiencing the issue:</gcds-text><ol><li>Navigate to the appropriate cluster that is experiencing the issue.<ul><li>Set your KUBECONFIG, or navigate to the cluster in Lens.</li></ul></li></ol><gcds-text character-limit=true>There will be a couple of different steps required to address the JobIncomplete Alert:</gcds-text><ol><li>Identify the root cause</li><li>Apply changes for remediation</li><li>Verify the Job succeeds</li></ol><gcds-heading tag=h4 id=1-identify-root-cause character-limit=true>1. Identify root cause</gcds-heading>
<gcds-text character-limit=true>In order to identify the root cause of the failed Job, analyze the Job resource to determine which errors may be occuring.
<code>kubectl -n &lt;namespace> describe job &lt;jobName></code></gcds-text>
<gcds-text character-limit=true>You may see some relevant information within the Job resource. Within the Job, you can determine the number of <em>pods</em> that have <em>Succeeded</em>, <em>Failed</em>, or are still <em>Running</em>.</gcds-text>
<gcds-text character-limit=true>Most importantly, navigate to the <em>Events</em> section of the resource. Here, you will see all <em>Informational</em>, <em>Warning</em>, and <em>Errors</em> messages associated with the Job. You may be able to identify the exact name of the <em>pods</em> the Job is using.</gcds-text>
<gcds-text character-limit=true>Alternatively, you could list all Pods that belong to a Job in a machine readable form, using a command similar to the following. You should see the list of Pods as the output:</gcds-text><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pods<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>kubectl get pods --selector<span style=color:#f92672>=</span>job-name<span style=color:#f92672>=</span>&lt;jobNameHere&gt; --output<span style=color:#f92672>=</span>jsonpath<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;{.items[*].metadata.name}&#39;</span><span style=color:#66d9ef>)</span>
</span></span><span style=display:flex><span>echo $pods
</span></span></code></pre></div><gcds-text character-limit=true>To further diagnose, you will need to analyze these pods! Analyze the pod logs for each of the Pods to help determine the root cause:</gcds-text>
<gcds-text character-limit=true><code>kubectl -n &lt;namespace> logs &lt;podName></code></gcds-text>
<gcds-text character-limit=true>Analyze the log contents for any errors, or even warnings, that may help diagnose the issue.</gcds-text>
<gcds-heading tag=h4 id=2-apply-changes-for-remediation character-limit=true>2. Apply changes for remediation</gcds-heading>
<gcds-text character-limit=true>Depending on the underlying root cause, there are a number of changes you may have to make to remediate the Alert.</gcds-text>
<gcds-text character-limit=true>Some examples of remediation may include:</gcds-text><ul><li>Modify the Job (or the CronJob) to edit values associated with
<gcds-link href=https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-termination-and-cleanup>Job termination and cleanup:</gcds-link><ul><li>restartPolicy,</li><li>backoffLimit: if you need to increase or decrease this threshold</li><li>activeDeadlineSeconds: if the Job fails for a <em>DeadlineExceeded</em> event</li></ul></li><li>Scaling up cluster resources - in cases where the Job fails due to lack of resources (Memory, CPU)</li></ul><gcds-heading tag=h4 id=3-verify-the-job-succeeds character-limit=true>3. Verify the Job succeeds</gcds-heading>
<gcds-text character-limit=true>Once the remediation steps have been implemented, the Job will continue its progress and attempt to correct itself. You will need to monitor the Job to ensure that the Job runs and completes successfully after applying your fix.</gcds-text>
<gcds-text character-limit=true>If remediation was successful, the Job should run to completion and finish its task.</gcds-text><ul><li>Verify that the Job has completed</li><li>Verify that the previous <em>JobIncomplete</em> Alerts now show up as &lsquo;Resolved&rsquo; in our various Alert channels</li><li>Verify that no new instances of the Alert occur for the same Job</li></ul><gcds-heading tag=h3 id=additional-troubleshooting-1 character-limit=true>Additional Troubleshooting</gcds-heading>
<gcds-text character-limit=true>If remediation was not successful, the Job may fail again. You may experience more instances of the <em>JobIncomplete</em> Alert if the root cause has not been fixed. Alternatively, different Alerts may fire based on the existence of any new errors or events.</gcds-text><ul><li>If you are still encountering the JobIncomplete error, simply re-iterate through the resolution process again to try and identify a new root cause, or to re-evaluate your previous attempt.</li><li>If new Alerts are being fired on the Job resource, you will need to refer to the Cluster or Namespace Alert&rsquo;s
<gcds-link href=/team/monitoring-alerts/prometheus/ title="Prometheus Documentation">Prometheus Runbook documentation</gcds-link> to determine the appropriate resolution process.</li></ul><gcds-heading tag=h2 id=alert-gitlabbackupincomplete character-limit=true>Alert: GitlabBackupIncomplete</gcds-heading>
<gcds-text character-limit=true>This alert occurs within <strong>gitlab</strong> namespace of the the Management cluster, and will be triggered when a gitlab-backup job is taking more than 24 hours to complete. To troubleshoot this alert, you will need to determine why the Job may be failing to complete successfully.</gcds-text>
<gcds-text character-limit=true>This alert could indicate an issue with the Job resource itself, or an underlying issue related to cluster resources, or Job scheduling etc.</gcds-text>
<gcds-text character-limit=true>As this alert is a less sensitive version of JobIncomplete alert, the resolution process may be similar to the steps outlined
<gcds-link href=#resolution-process>above</gcds-link>.</gcds-text>
<gcds-date-modified lang=en>0001-01-01</gcds-date-modified></section></gcds-grid></gcds-container></main><gcds-footer lang=en display=compact contextual-heading=Aurora contextual-links='{ "Contact us": "/contact","Report an issue": "https://github.com/gccloudone/aurora/issues/new/choose","About us": "/about" }'></gcds-footer><script src=https://cdn.jsdelivr.net/npm/glightbox/dist/js/glightbox.min.js></script><script>const style=document.createElement("style");style.innerHTML=`
    .glightbox-container.glightbox-custom-white-bg img {
      background: white !important;
    }
  `,document.head.appendChild(style);const lightbox=GLightbox({openEffect:"zoom",closeEffect:"fade",skin:"custom-white-bg"})</script></body></html>