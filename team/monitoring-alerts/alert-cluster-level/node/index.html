<!doctype html><html dir=ltr lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=5"><title>Node Alerts | Aurora
</title><link rel=icon href=/favicon.ico type=image/x-icon><link rel=preload href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css as=style crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.design-system.alpha.canada.ca/@cdssnc/gcds-utility@1.10.0/dist/gcds-utility.min.css><link rel=stylesheet href=https://cdn.design-system.alpha.canada.ca/@cdssnc/gcds-components@0.42.0/dist/gcds/gcds.css><script async type=module src=https://cdn.design-system.alpha.canada.ca/@cdssnc/gcds-components@0.42.0/dist/gcds/gcds.esm.js></script><link rel=stylesheet href=/scss/main.css><link rel=stylesheet href=/scss/custom.css><style>html{font-size:16px}body{font-size:var(--gcds-font-sizes-text);line-height:var(--gcds-line-heights-text);color:var(--gcds-color-grayscale-1000);font-family:var(--gcds-font-families-body),sans-serif;margin:0;animation:fade .05s forwards ease-in-out}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/glightbox/dist/css/glightbox.min.css></head><body style=margin:0 class=character-limit><main><gcds-container centered size=md><gcds-text size=caption style="text-align: center;"><strong>Alpha</strong>&nbsp;&nbsp;This is an experimental service.</gcds-text>
</gcds-container><gcds-header lang=en lang-href signature-variant=colour skip-to-href=#><gcds-search lang=en slot=search action=/sr/srb.html placeholder=Aurora></gcds-search><gcds-top-nav slot=menu label="Top navigation" alignment=right><gcds-nav-link href=/ slot=home>GC Cloud One: Aurora
</gcds-nav-link><gcds-nav-group open-trigger=Platform menu-label=Platform><gcds-nav-link href=/proposal>Proposal
</gcds-nav-link><gcds-nav-link href=/vision>Vision
</gcds-nav-link><gcds-nav-link href=/architecture>Architecture
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger=Team menu-label=Team><gcds-nav-link href=/team/sop>Standard Operating Procedures
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/>Monitoring
</gcds-nav-link><gcds-nav-link href=/team/appendix>Appendix
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger=Community menu-label=Community><gcds-nav-link href=/get-involved>Get involved
</gcds-nav-link><gcds-nav-link href=/rules-of-engagement>Rules of Engagement
</gcds-nav-link><gcds-nav-link href=/technical-advisory-group>Technical Advisory Group
</gcds-nav-link></gcds-nav-group><gcds-nav-link href=/contact slot>Contact us
</gcds-nav-link></gcds-top-nav><gcds-breadcrumbs slot=breadcrumb><gcds-breadcrumbs-item href="https://hosting-services-hebergement.canada.ca/s/gc-cloud-one?language=en_US">GC Cloud One</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/>Aurora</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/team/>Team Guide</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/team/monitoring-alerts/>Monitoring and Alerting</gcds-breadcrumbs-item>
<gcds-breadcrumbs-item href=/team/monitoring-alerts/alert-cluster-level/>Cluster Level Alerts</gcds-breadcrumbs-item>
</gcds-breadcrumbs></gcds-header><gcds-container size=xl main-container centered tag=main><gcds-grid tag=section columns=1fr columns-desktop="
        1fr 3fr
      " align-items=start class=hydrated><aside role=complementary><gcds-side-nav label="Team Guide"><gcds-nav-link href=/team/>Team Guide
</gcds-nav-link><gcds-nav-group open-trigger="Standard Operating Procedures" menu-label="Standard Operating Procedures"><gcds-nav-link href=/team/standard-operating-procedures/>Standard Operating Procedures
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/backup-disaster-recovery/>Backup and Disaster Recovery
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/bootstrap-cluster/>Bootstrap Cluster
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/infrastructure-and-configuration-management/>Infrastructure and Configuration Management
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/issue-naming/>Issue and PR Naming Conventions
</gcds-nav-link><gcds-nav-link href=/team/standard-operating-procedures/enterprise-landing-zone/>Onboarding process for the Enterprise Landing Zone
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger="Monitoring and Alerting" menu-label="Monitoring and Alerting" open><gcds-nav-link href=/team/monitoring-alerts/>Monitoring and Alerting
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/prometheus/>Prometheus
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/prometheus-operator/>Prometheus Operator
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alertmanager/>Alertmanager
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/blackbox-exporter/>Blackbox Exporter
</gcds-nav-link><gcds-nav-group open-trigger="Cluster Level Alerts" menu-label="Cluster Level Alerts" open><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/>Cluster Level Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/cert-manager/>Cert Manager Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/node/ current=true>Node Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/node-pool-pod-capacity/>Node Pool Capacity Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/probe-failure/>Probe Failure Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/prometheus-storage-low/>Prometheus Storage Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/ssl-cert-expiring-soon/>SSL Certificate Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-cluster-level/velero/>Velero Alerts
</gcds-nav-link></gcds-nav-group><gcds-nav-group open-trigger="Namespace Level Alerts" menu-label="Namespace Level Alerts"><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/>Namespace Level Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/container/>Container Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/job/>Job Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/persistent-volume-claims/>Persistent Volume Claim Alerts
</gcds-nav-link><gcds-nav-link href=/team/monitoring-alerts/alert-namespace-level/pod-not-ready/>Pod Alerts
</gcds-nav-link></gcds-nav-group></gcds-nav-group><gcds-nav-group open-trigger=Appendix menu-label=Appendix><gcds-nav-link href=/team/appendix/>Appendix
</gcds-nav-link><gcds-nav-link href=/team/appendix/acronyms/>Acronyms
</gcds-nav-link><gcds-nav-link href=/team/appendix/lexicon/>Lexicon</gcds-nav-link></gcds-nav-group></gcds-side-nav></aside><section class=mb-400><gcds-heading tag=h1>Node Alerts</gcds-heading>
<gcds-alert alert-role=danger container=full heading="Avis de traduction" hide-close-btn=true hide-role-icon=false is-fixed=false class="hydrated mb-400"><gcds-text>Veuillez noter que ce document est actuellement en cours de développement actif et pourrait être sujet à des révisions. Une fois terminé, il sera entièrement traduit en français et mis à disposition dans sa version finale.</gcds-text>
</gcds-alert><gcds-heading tag=h2 id=node-pressure-eviction-alerts character-limit=true>Node Pressure Eviction Alerts</gcds-heading>
<gcds-text character-limit=true>Node pressure eviction alerts are configured to occur at the cluster level. As defined in the
<gcds-link href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/>official Kubernetes documentation</gcds-link>, &ldquo;node-pressure eviction is the process by which the kubelet proactively terminates pods to reclaim resources on nodes&rdquo;. When a particular node resource such as the memory or disk space reaches a certain threshold, the kubelet can terminate one or more of the pods on the node to reclaim resources and prevent starvation.</gcds-text>
<gcds-text character-limit=true>The kubelet has the following default hard eviction thresholds:</gcds-text><ul><li>memory.available&lt;100Mi</li><li>nodefs.available&lt;10%</li><li>imagefs.available&lt;15%</li><li>nodefs.inodesFree&lt;5%</li></ul><gcds-heading tag=h2 id=alert-nodediskpressure character-limit=true>Alert: NodeDiskPressure</gcds-heading>
<gcds-text character-limit=true>This alert is triggered when the available disk space and inodes on either the node&rsquo;s root filesystem or image filesystem has satisfied eviction thresholds. When this happens, Kubernetes terminates pod(s) to reduce the load on the nodes. Doing this can cause issues to the applications on the node if the application doesn&rsquo;t know how to handle a sudden shutdown properly.</gcds-text>
<gcds-text character-limit=true>There are two primary reasons that could cause this event. Either Kubernetes has not cleaned up any unused images, or more commonly, the disk pressure is caused by logs or application data filling up the disk space. To determine the issue, a good place to start is to find out which files are taking up the greatest amount of disk space. This can be accomplished by SSHing into the node and running the <code>df -h</code> & <code>du -h</code> commands. If it is found that the issue isn&rsquo;t caused by any unexpected behavior and all the data is necessary, consider increasing the size of the disk. Otherwise, if there is data on the disk that is no longer necessary to keep, delete it and if there is unexpected behavior debug the application issue.</gcds-text>
<gcds-heading tag=h2 id=alert-nodememorypressure character-limit=true>Alert: NodeMemoryPressure</gcds-heading>
<gcds-text character-limit=true>This alert is triggered when the available memory on the node has satisfied an eviction threshold. When troubleshooting these issue the following should be considered:</gcds-text><ul><li>Ensure memory requests and limits are set for each container within the node.</li><li>If resource requests and limits have already been configured, consider if the memory requests needs to be increased.</li><li>Evaluate the memory usage of pods within the node. Ensure that applications are consuming memory efficiently. Memory usage can be viewed in Grafana dashboards or graphs of Prometheus queries.</li><li>Consider if the node SKU can handle the resources requested by the workloads efficiently. In other words evaluate if the nodes running the workloads are tuned/appropriately fit the resources required to run the workloads.</li></ul><gcds-text character-limit=true>This alert can be mitigated by setting request and limit resource settings on each container within the node. Using these attributes, one can set the amount of CPU and memory needed by the container and prevent the container from using more resources than the value set by the limit attribute. Setting a resource request will help the Kubernetes scheduler choose a node for the pod to run on since it won&rsquo;t schedule a pod onto a node that has less available resources than what is specified in the resource request. If the container starts to consume more memory than the configured resource limit, then the pod is terminated. When the container starts to consume more CPU then the configured resource limit, the CPU is throttled. Refer to the
<gcds-link href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>official documentation on requests and limits</gcds-link> for more information.</gcds-text>
<gcds-heading tag=h2 id=alert-nodepidpressure character-limit=true>Alert: NodePIDPressure</gcds-heading>
<gcds-text character-limit=true>This alert is triggered when the available processes identifiers on the (Linux) node has fallen below an eviction threshold. In other words, it occurs when a container spawns too many processes and starves the node of available process IDs. This typically means one or more of the applications running on containers within the node is not functioning as intended and thus it needs to be examined. To prevent the node from running out of availble PIDs, one can also set a PID limit for each pod.</gcds-text>
<gcds-heading tag=h2 id=alert-nodeunschedulable character-limit=true>Alert: NodeUnschedulable</gcds-heading>
<gcds-text character-limit=true>This alert occurs at the cluster level. This alert is triggered when a node belonging to the cluster has been in the Unschedulable state for over 1 hour. The alert specifies the cluster and node name for troubleshooting.</gcds-text>
<gcds-text character-limit=true>A node may be Unschedulable for a number of reasons:</gcds-text><ul><li>The node may be unhealthy,</li><li>The node may have been cordoned, marking the node as unschedulable.</li></ul><gcds-heading tag=h3 id=resolution-process character-limit=true>Resolution Process</gcds-heading>
<gcds-text character-limit=true>The resolution process will be different depending on the root cause of the unschedulable node.</gcds-text><ol><li><gcds-text character-limit=true>Verify the status of the Node in the cluster:</gcds-text>
<gcds-text character-limit=true><code>kubectl get nodes</code></gcds-text><ul><li><gcds-text character-limit=true>If the status of the node is <em>Ready,SchedulingDisabled</em>, Uncordon the node:</gcds-text>
<gcds-text character-limit=true><code>kubectl uncordon &lt;nodeName></code></gcds-text></li><li><gcds-text character-limit=true>If the status of the node is anything else:</gcds-text><ul><li>Determine if any other Alerts are being triggered by the node. Cross reference Alerts with existing
<gcds-link href=/team/monitoring-alerts/alert-cluster-level/>Cluster Alert Runbooks</gcds-link> to resolve issues accordingly.</li></ul></li></ul></li><li><gcds-text character-limit=true>After resolving the underlying issue, verify that the node is now schedulable. The node should be in the <em>Ready</em> state:</gcds-text>
<gcds-text character-limit=true><code>kubectl get nodes</code></gcds-text></li></ol><gcds-heading tag=h3 id=additional-troubleshooting character-limit=true>Additional Troubleshooting</gcds-heading><ul><li><gcds-text character-limit=true>For more detailed information, you can always describe the node:</gcds-text>
<gcds-text character-limit=true><code>kubectl describe node &lt;nodeName></code></gcds-text></li></ul><gcds-date-modified lang=en>0001-01-01</gcds-date-modified></section></gcds-grid></gcds-container></main><gcds-footer lang=en display=compact contextual-heading=Aurora contextual-links='{ "Contact us": "/contact","Report an issue": "https://github.com/gccloudone/aurora/issues/new/choose","About us": "/about" }'></gcds-footer><script src=https://cdn.jsdelivr.net/npm/glightbox/dist/js/glightbox.min.js></script><script>const style=document.createElement("style");style.innerHTML=`
    .glightbox-container.glightbox-custom-white-bg img {
      background: white !important;
    }
  `,document.head.appendChild(style);const lightbox=GLightbox({openEffect:"zoom",closeEffect:"fade",skin:"custom-white-bg"})</script></body></html>